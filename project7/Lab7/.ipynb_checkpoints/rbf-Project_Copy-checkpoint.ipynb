{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAME HERE**\n",
    "\n",
    "Spring 2021\n",
    "\n",
    "CS 251: Data Analysis and Visualization\n",
    "\n",
    "Project 7: Radial Basis Function Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: RBF Network development dataset\n",
    "\n",
    "The eventual goal is to train a neural network so that it learns to recognize which human handwritten digit is shown in an image  (i.e. the numbers 0, 1, ..., 9). Before doing this, you will use simpler data to develop and debug your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Load and preprocess data\n",
    "\n",
    "- Load in the `rbf_dev_train.csv` and `rbf_dev_test.csv` train and test sets.\n",
    "- For the train and test sets, separate the data (*X* and *Y*) from the class values (*class*). The test code below assumes the following names:\n",
    "    - `y_train` and `y_test` for the class values in the train and test sets, respectively.\n",
    "    - `rbf_dev_train` and `rbf_dev_test` for the train and test set data, respectively.\n",
    "- Normalize each data feature between 0 and 1 (based on each feature's dynamic range).\n",
    "    - Use the `min` and `max` values derived from the training set to normalize the test set. *This will ensure that identical feature values in the training and test sets get normalized to the same numeric value.*\n",
    "\n",
    "**Tip:** It might be a good idea to write a few functions below so that you don't have a lot of redundant code between train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dev_ds(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    x = df[['X', 'Y']].to_numpy()\n",
    "    y = df['class'].to_numpy()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(x, mins, maxs):\n",
    "    return (x - mins) / (maxs - mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/rbf_dev_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-77d15a5fbe45>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrbf_dev_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_dev_ds\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'data/rbf_dev_train.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mrbf_dev_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_dev_ds\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'data/rbf_dev_test.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-1ee10798665f>\u001B[0m in \u001B[0;36mload_dev_ds\u001B[0;34m(filename)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mload_dev_ds\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'X'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'Y'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'class'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/gpu_datasci/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    609\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 610\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    611\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    612\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/gpu_datasci/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    460\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    461\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 462\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    463\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    464\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/gpu_datasci/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    817\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    818\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 819\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    820\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    821\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/gpu_datasci/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1048\u001B[0m             )\n\u001B[1;32m   1049\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1050\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1051\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1052\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/gpu_datasci/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m   1865\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1866\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1867\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1868\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1869\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"storage_options\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"encoding\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"memory_map\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"compression\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/gpu_datasci/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m   1360\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHanldes\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1361\u001B[0m         \"\"\"\n\u001B[0;32m-> 1362\u001B[0;31m         self.handles = get_handle(\n\u001B[0m\u001B[1;32m   1363\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1364\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/gpu_datasci/lib/python3.8/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    640\u001B[0m                 \u001B[0merrors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"replace\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    641\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 642\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    643\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    644\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/rbf_dev_train.csv'"
     ]
    }
   ],
   "source": [
    "rbf_dev_train, y_train = load_dev_ds('data/rbf_dev_train.csv')\n",
    "rbf_dev_test, y_test = load_dev_ds('data/rbf_dev_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_min = np.min(rbf_dev_train, axis=0)\n",
    "train_max = np.max(rbf_dev_train, axis=0)\n",
    "rbf_dev_train = normalize_features(rbf_dev_train, train_min, train_max)\n",
    "rbf_dev_test = normalize_features(rbf_dev_test, train_min, train_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing test code\n",
    "\n",
    "The following test code is a good sanity check, but you are encouoraged to do additional testing to make sure that your preprocessing pipeline is working properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Your training set is an ndarray? {isinstance(rbf_dev_train, np.ndarray)}')\n",
    "print(f'Your training classes is an ndarray? {isinstance(y_train, np.ndarray)}')\n",
    "print(f'Your test set is an ndarray? {isinstance(rbf_dev_test, np.ndarray)}')\n",
    "print(f'Your test classes is an ndarray? {isinstance(y_test, np.ndarray)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Your training set shape is {rbf_dev_train.shape} and should be (1600, 2).')\n",
    "print(f'Your training classes shape is {y_train.shape} and should be (1600,).')\n",
    "print(f'Your test set shape is {rbf_dev_test.shape} and should be (400, 2).')\n",
    "print(f'Your test classes shape is {y_test.shape} and should be (400,).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check dtypes (detailed)\n",
    "\n",
    "Remove from assigned project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Your training set is an float? {isinstance(rbf_dev_train[0,0], float)}')\n",
    "print(f'Your training classes is an int? {isinstance(y_train[0], np.int64)}')\n",
    "print(f'Your test set is an float? {isinstance(rbf_dev_test[0,0], float)}')\n",
    "print(f'Your test classes is an int? {isinstance(y_test[0], np.int64)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check normalization (detailed)\n",
    "\n",
    "Remove from assigned project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Your training set mins/maxs: {rbf_dev_train.min(axis=0)}/{rbf_dev_train.max(axis=0)}\\nand should be                [0. 0.]/[1. 1.]')\n",
    "print(f'Your test set mins/maxs: {rbf_dev_test.min(axis=0)}/{rbf_dev_test.max(axis=0)}\\nand should be            [0.02038 0.01194]/[0.99805 0.98749]')\n",
    "\n",
    "print(f'Training values look ok? {len(np.unique(rbf_dev_train)) > 2}')\n",
    "print(f'Test values look ok? {len(np.unique(rbf_dev_test)) > 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Plot data\n",
    "\n",
    "Create a scatter plot of the training data in the cell below.\n",
    "\n",
    "- Color code each sample by its output class.\n",
    "- Make the aspect ratio (height and width) of the x and y axes in the figure equal.\n",
    "\n",
    "If everything is working properly, you should see a jack-o-lantern whose eyes, noise, mouth, and stem are colored differently than the rest of the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_c1 = rbf_dev_train[y_train == 0]\n",
    "x_c2 = rbf_dev_train[y_train == 1]\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(x_c1[:, 0], x_c1[:, 1], '.')\n",
    "plt.plot(x_c2[:, 0], x_c2[:, 1], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Radial basis function neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rbf_net_lab import RBF_Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Network initalization\n",
    "\n",
    "You will use K-means to initialize the RBF hidden layer prototypes and sigmas.\n",
    "\n",
    "1. Copy over your `kmeans.py` from your previous project. If your K-means clustering code isn't working properly, you may use `scipy` functions in this project instead, but at a 1 point reduction. Check out `scipy.cluster.vq.kmeans`.\n",
    "\n",
    "2. Implement the method templates in `rbf.py` that initialize the hidden layer of the neural network:\n",
    "    - Finish writing the constructor\n",
    "    - `get_num_hidden_units`\n",
    "    - `get_num_output_units`\n",
    "    - `avg_cluster_dist`: Compute the average distance between each cluster center found by K-means and all the points assigned to the same cluster.\n",
    "    - `initialize`: Use K-means to set the Gaussian hidden unit centers (**prototypes**) and standard deviations (**sigmas**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `avg_cluster_dist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# N = 10 samples, M = 5 features\n",
    "test_data = np.random.normal(size=(10, 5))\n",
    "# 4 hidden units / prototypes, each has 5 features\n",
    "test_centroids = np.random.normal(size=(4, 5))\n",
    "# Each sample assigned to one of 4 hidden unit prototypes\n",
    "test_assignments = np.random.randint(low=0, high=4, size=(10,))\n",
    "\n",
    "test_net = RBF_Net(4, 3)\n",
    "print(f'Number of hidden units in your net is {test_net.get_num_hidden_units()} and should be 4')\n",
    "print(f'Number of output units in your net is {test_net.get_num_output_units()} and should be 3')\n",
    "test_clust_mean_dists = test_net.avg_cluster_dist(test_data, test_centroids, test_assignments)\n",
    "\n",
    "print(f'Your avg within cluster distances are\\n{test_clust_mean_dists} and should be\\n[2.23811 3.94891 3.12267 3.4321]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `initialize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net.initialize(test_data)\n",
    "\n",
    "print(f'Your prototypes have shape {test_net.get_prototypes().shape} and the shape should be (4, 5).')\n",
    "print(f'Your hidden unit sigmas have shape {test_net.sigmas.shape} and the shape should be (4,).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test initialization methods\n",
    "\n",
    "In the cell below, write test code for your RBF network initialization:\n",
    "- Create a new RBF network with 7 hidden units and 2 output classes.\n",
    "- Call the `initalize` method on it, passing in the training data.\n",
    "- Create a class color-coded scatterplot of the training data with an equal axis aspect ratio, like above, now with the prototypes clearly marked with a different marker and/or color.\n",
    "\n",
    "You should see fairly evenly distributed prototypes, with one in most, if not all, \"pockets\" of a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_hidden_units = 7\n",
    "num_classes = 2\n",
    "\n",
    "net = RBF_Net(num_classes=num_classes, num_hidden_units=num_hidden_units)\n",
    "net.initialize(rbf_dev_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "centroids = net.get_prototypes()\n",
    "x_c1 = rbf_dev_train[y_train == 0]\n",
    "x_c2 = rbf_dev_train[y_train == 1]\n",
    "plt.plot(x_c1[:, 0], x_c1[:, 1], '.')\n",
    "plt.plot(x_c2[:, 0], x_c2[:, 1], '.')\n",
    "plt.plot(centroids[:, 0], centroids[:, 1], '*', c='k', markersize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Do you think the prototypes enable the RBF network to learn the data well? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** Yes. They are well distributed across the data space and fall in clusters of single classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Compute hidden and output layer activations\n",
    "\n",
    "Implement the following methods so that you can train your RBF network:\n",
    "- `hidden_act`: Computes hidden layer activation values: Determines the similarity between hidden layer prototypes with the input data.\n",
    "- `output_act`: Computes output layer activation values: Multiply hidden unit activation by output unit weights.\n",
    "\n",
    "**Hidden unit activation**:\n",
    "\n",
    "The activation of hidden unit $j$ to data sample $i$ is computed according to $$H_{ij} = \\exp \\left (-\\frac{||\\vec{d_i} - \\vec{\\mu_j} ||^2}{2\\sigma_j^2 + \\epsilon}\\right )$$ \n",
    "where $\\vec{d_i}$ is the data sample, $\\vec{\\mu_j}$ is the prototype (center) of the hidden unit, $\\sigma_j$ is the hidden unit's standard deviation, $\\epsilon$ is a small number (e.g. 1e-8), and $|| \\cdot ||^2$ is the **squared** Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test hidden_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# N = 3 samples, M = 5 features\n",
    "test_data = np.random.normal(size=(3, 5))\n",
    "# 4 hidden units / prototypes, each has 5 features\n",
    "test_centroids = np.random.normal(size=(4, 5))\n",
    "# Each sample assigned to one of 4 hidden unit prototypes\n",
    "test_sigmas = np.random.uniform(size=(4,))\n",
    "test_wts = 2*np.random.uniform(size=(4+1, 3)) - 1\n",
    "\n",
    "test_net = RBF_Net(4, 3)\n",
    "test_net.prototypes = test_centroids\n",
    "test_net.sigmas = test_sigmas\n",
    "test_net.wts = test_wts\n",
    "test_h_act = test_net.hidden_act(test_data)\n",
    "print(f'Your hidden layer activation is\\n{test_h_act}\\n\\nand should be')\n",
    "print('[[0.      0.      0.00009 0.00033]\\n [0.00013 0.      0.00004 0.00014]\\n [0.      0.      0.      0.00001]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test output_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out_act = test_net.output_act(test_h_act)\n",
    "print(f'Your output layer activation is\\n{test_out_act}\\n\\nand should be')\n",
    "print('[[-0.72136  0.61505 -0.20481]\\n [-0.72151  0.61487 -0.20466]\\n [-0.72144  0.61479 -0.20465]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Learn network weights using linear regression (CS251)\n",
    "\n",
    "To train your RBF network, you will need to find optimal weights between hidden and output layer neurons to allows your network to accurately classify the training data (i.e. learn from the data). An efficient solution is to use linear regression to solve a least square problem: minimizing the squared difference between the *hidden layer activations* and the *true data classes*.\n",
    "\n",
    "- In `rbf.py`, implement `linear_regression(A, y)`. To do this, you may adapt your code in `linear_regression`, use `numpy.linalg.lstsq()`, or use `scipy.linalg.lstsq()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test linear regression\n",
    "\n",
    "Running the following test code should generate a familar regression fit to the Iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv')\n",
    "iris = df[['sepal_length', 'petal_width']].to_numpy()\n",
    "\n",
    "num_hidden_units = 1\n",
    "num_classes = 4\n",
    "net = RBF_Net(num_classes=num_classes, num_hidden_units=num_hidden_units)\n",
    "iris_x = np.reshape(iris[:, 0], [len(iris), 1])\n",
    "iris_y = np.reshape(iris[:, 1], [len(iris), 1])\n",
    "iris_c = net.linear_regression(iris_x, iris_y)\n",
    "\n",
    "line_x = np.linspace(iris_x.min(), iris_x.max())\n",
    "line_y = line_x * iris_c[0] + iris_c[1]\n",
    "plt.scatter(iris_x, iris_y)\n",
    "plt.plot(line_x, line_y)\n",
    "plt.title('Iris — Linear Regression test')\n",
    "plt.xlabel('sepal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Learn network weights using linear regression (CS252)\n",
    "\n",
    "To train your RBF network, you will need to find optimal weights between hidden and output layer neurons to allows your network to accurately classify the training data (i.e. learn from the data). An efficient solution is to use linear regression to solve a least square problem: minimizing the squared difference between the *hidden layer activations* and the *true data classes*.\n",
    "\n",
    "You will implement a SVD-based linear regression method, which is both fast and numerically accurate.\n",
    "\n",
    "- In `rbf.py`, implement `linear_regression` using the SVD-based method. The weights $c$ can be solved via the following matrix equation: $$c = A^+y$$ where $A^+$ is the pseudo inverse of the matrix of RBF hidden layer activations $A$ (*data matrix*) and the correct classes $y$. You can use `np.linalg.svd` but you should implement this without `np.linalg.pinv` (but you can use it to help you debug/check your work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test SVD linear regression\n",
    "\n",
    "Running the following test code should generate a familar regression fit to the Iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv')\n",
    "iris = df[['sepal_length', 'petal_width']].to_numpy()\n",
    "\n",
    "num_hidden_units = 1\n",
    "num_classes = 4\n",
    "net = RBF_Net(num_classes=num_classes, num_hidden_units=num_hidden_units)\n",
    "iris_x = np.reshape(iris[:, 0], [len(iris), 1])\n",
    "iris_y = np.reshape(iris[:, 1], [len(iris), 1])\n",
    "iris_c = net.linear_regression(iris_x, iris_y)\n",
    "\n",
    "line_x = np.linspace(iris_x.min(), iris_x.max())\n",
    "line_y = line_x * iris_c[0] + iris_c[1]\n",
    "plt.scatter(iris_x, iris_y)\n",
    "plt.plot(line_x, line_y)\n",
    "plt.title('Iris — SVD regression test')\n",
    "plt.xlabel('sepal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Train your RBF Network\n",
    "\n",
    "Implement the following methods then train your neural network! In the cell below, train a RBF network with 10 hidden units on the RBF dev dataset. **If everything is working, you should get 88-89% accuracy on the training and test set and 89-93% on the test set.**\n",
    "\n",
    "- `train`: Determine output layer weights using linear regression to classify data.\n",
    "- `predict`: Use trained network (after learning) to predict the class of data.\n",
    "- `accuracy`: Compute the accuracy by comparing the network predicted and actual class for each data sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_hidden_units = 10\n",
    "num_classes = 2\n",
    "\n",
    "net = RBF_Net(num_classes=num_classes, num_hidden_units=num_hidden_units)\n",
    "net.train(rbf_dev_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set accuracy\n",
    "y_pred = net.predict(rbf_dev_train)\n",
    "net.accuracy(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set accuracy\n",
    "y_pred_test = net.predict(rbf_dev_test)\n",
    "net.accuracy(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_dev_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Handwritten digit classification: MNIST data\n",
    "\n",
    "You will train a RBF network on a \"real\" image dataset of handwritten number digits:\n",
    "- 60,000 images in training set, 10,000 images in test set.\n",
    "- Each image is 28x28 pixels.\n",
    "- The images are grayscale (no RGB colors).\n",
    "- Each image (data sample) contains ONE of 10 numeric digit $0, 1, 2, \\ldots, 8, 9$.\n",
    "\n",
    "The goal is to train your network so that it can correctly predict the numeric digit in an image.\n",
    "\n",
    "More information about MNIST: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Load in and look at MNIST dataset\n",
    "\n",
    "- Use the numpy function `load` to load in the MNIST train/test data and the associated class labels.\n",
    "- Create a 5x5 grid showing the first 25 images in the dataset. It should \"look good\" (e.g. turn off tick marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('data/mnist_train_data.npy')\n",
    "y_train = np.load('data/mnist_train_labels.npy')\n",
    "x_test = np.load('data/mnist_test_data.npy')\n",
    "y_test = np.load('data/mnist_test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Your training set shape is {x_train.shape} and should be (60000, 28, 28).')\n",
    "print(f'Your training classes shape is {y_train.shape} and should be (60000,).')\n",
    "print(f'Your test set shape is {x_test.shape} and should be (10000, 28, 28).')\n",
    "print(f'Your test classes shape is {y_test.shape} and should be (10000,).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = rows = 5\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols)\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        axes[i, j].imshow(x_train[i*rows + j])\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Preprocess data\n",
    "\n",
    "- Flatten non-leading dimensions of the train and test sets. For example, the training set should go from (60000, 28, 28) to (60000, 784). **Do not hard code this!** Your code should work for any data with three dimensions.\n",
    "- Normalize so that the maximum possible value in each image is 1 (and the minimum possible is 0) by dividing by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, [len(x_train), -1])\n",
    "x_test = np.reshape(x_test, [len(x_test), -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Train and assess performance on MNIST\n",
    "\n",
    "Train a RBF network on MNIST. **Your goal is to get >=90% accuracy on both train and test sets.**\n",
    "\n",
    "**Tips:**\n",
    "- Depending on your laptop or machine you are using, training could take many hours if you use the full 60,000 sample training set. Select a subset to train on (e.g. ~5000) that takes a reasonable amount of time (e.g. minutes). You should be able to hit your accuracy goals without too much data, effort, or time.\n",
    "- Use the code below to visualize your hidden layer prototypes to help with debugging (assumes your network is called `mnist_net`).\n",
    "- Do not pare down the test set (i.e. it should remain at 10,000 samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_units = 150\n",
    "num_classes = 10\n",
    "n = 5000\n",
    "\n",
    "mnist_net = RBF_Net(num_classes=num_classes, num_hidden_units=num_hidden_units)\n",
    "mnist_net.train(x_train[:n], y_train[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train acc\n",
    "mnist_train_y_pred = mnist_net.predict(x_train[:n])\n",
    "mnist_net.accuracy(mnist_train_y_pred, y_train[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test acc\n",
    "mnist_test_y_pred = mnist_net.predict(x_test)\n",
    "mnist_net.accuracy(mnist_test_y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network hidden layer prototypes\n",
    "prototypes = mnist_net.get_prototypes()\n",
    "prototypes = np.reshape(prototypes, [prototypes.shape[0], 28, 28])\n",
    "\n",
    "cols = rows = 5\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols)\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        axes[i, j].imshow(prototypes[i*rows + j])\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Which part of the training process seems to take the longest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:** K-means to set the initial centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** What accuracy did you achieve on the training set? List all parameters that you needed to set (e.g. number of training samples, number hidden units, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3:** I got 94% with N=5000 and 150 hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Using the same parameters to get the training accuracy that you reported in Question 3, what test accuracy did you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4:** I got 92.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Visualize prototypes using network weights (CS252 only)\n",
    "\n",
    "Now that you've trained the network to recognize handwritten digits, let's visualize the dominant factors that the network learns to rely on to make each prediction.\n",
    "\n",
    "Create a 10 x 5 plot showing images of the 5 \"most influential hidden layer prototypes\" that contribute to predicting each of the 10 output classes. These are defined as the hidden layer prototypes that have the 5 largest weights going from the hidden layer to the output layer. *Recall that each hidden unit prototype has the same shape as a single data sample so each image in your grid should be 28x28.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "C = 10\n",
    "fig, axes = plt.subplots(nrows=C, ncols=n, figsize=(8, 8))\n",
    "for c in range(C):\n",
    "    # Get the indices of the top n prototypes\n",
    "    # with the highest absolute wts going to output neuron c\n",
    "    proto_inds = np.argpartition(mnist_net.wts[:,c], -n)[::-1][:n]\n",
    "    for i, ind in enumerate(proto_inds):\n",
    "        axes[c, i].imshow(mnist_net.prototypes[ind].reshape(28, 28))\n",
    "        axes[c, i].set_xticks([])\n",
    "        axes[c, i].set_yticks([])\n",
    "        \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Interpret what each row of your 10x5 grid of images means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5:** The learned network weights reflect the degree to which each of these hidden unit prototypes contribute to how strongly each output unit \"votes\" for each class. So each row shows us the prototypes that are weighted most heavily to generate strong predictions for a particular class. For example, row 1 shows us the prototypes that if closely resemble the input pattern, will contribute to a high activation value in the class 0 output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Are each of the images shown in your plot actual samples in the dataset?\n",
    "\n",
    "*Hint: Think about how the prototypes are created.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 6:** No, not necessarily. The prototypes are the result of running clustering (i.e. centroids from K-means) on the dataset and are not necessarily actual data samples, but they are embedded in the same feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "To receive credit for any extension, you must:\n",
    "- Not modify / prevent any code from the core project from working (e.g. make a copy before changing). In other words, **the notebook test code should still work!**\n",
    "- **You must describe what you did and what you found in detail**. This includes a summary of parameter values used in your simulations.\n",
    "- Include (*labeled!*) plots and/or numbers to present your results.\n",
    "- Write up your extensions below or in a separate notebook.\n",
    "\n",
    "**Rule of thumb: one deep, thorough extension is worth more than several quick, shallow extensions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Improve performance on MNIST with PCA\n",
    "\n",
    "Using all 768 features (pixels) in each image may not be very helpful for classification. For example, pixels around the border are almost always white. Transform the dataset(s) using PCA to compress the number of features before training your RBF network. Experiment with PCA to improve classification accuracy and runtime performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Explore parameter space\n",
    "\n",
    "When training your RBF network on MNIST, you had to pick some parameter values to use by hand. Pick one or more parameters and systematically vary them to quantify their effect on accuracy and simulation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Initialization schemes\n",
    "\n",
    "Research, test out, and quantify the performance of different techniques to set the hidden unit prototypes and sigmas. For example, an alternative way to initialize the prototypes is to perform K-means to cluster each class *separately*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) RBF networks for regression\n",
    "\n",
    "You used RBF networks to classify data, but you can use them to predict continuous values of a variable (e.g. stock market price). The key is to set the number of output numbers to the dimensionality of the data you are doing regression on (e.g. 1 for data with one feature). Do regression with your neural network on a dataset of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Other datasets\n",
    "\n",
    "Use your RBF network to classify other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) SVD-based linear regression (CS251)\n",
    "\n",
    "Implement a SVD-based linear regression method, which is both fast and numerically accurate. In the equation $Ac = y$ The weights $c$ can be solved via the following matrix equation: $$c = A^+y$$ where $A^+$ is the pseudo inverse of the matrix of RBF hidden layer activations $A$ (*data matrix*) and the correct classes $y$.\n",
    "\n",
    "Relying on numpy to figure out the pseudoinverse would be a mini extension, computing the pseudoinverse yourself would be a larger extension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}