{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAME HERE**\n",
    "\n",
    "Spring 2021\n",
    "\n",
    "CS 251: Data Analysis and Visualization\n",
    "\n",
    "Project 4: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import pca_cov\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "In your implementations, only the following \"high level\" `scipy`/`numpy` functions can be used:\n",
    "\n",
    "- `np.linalg.eig`\n",
    "\n",
    "**NOTE:** The numpy functions that you have been using so far are fine to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Principal component analysis using the covariance matrix\n",
    "\n",
    "In this task, you will implement principal component analysis (PCA) using the covariance matrix method, test your code, plot the results on the Iris dataset, then run PCA and analyze on several other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Import Iris data\n",
    "\n",
    "- In the below cell, load in the Iris dataset into a pandas DataFrame (note, this version of iris does not have the data type row — going forward, csv files we work with won't have this `type` row).\n",
    "- Print out the head (only showing the first 5 data samples).\n",
    "- Create an `PCA_COV` object called `pca` based on the DataFrame object that you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should print something that looks like this (with fancier formatting):\n",
    "\n",
    "    sepalLength\tsepalWidth\tpetalLength\tpetalWidth\tspecies\n",
    "    0\t5.1\t3.5\t1.4\t0.2\t0\n",
    "    1\t4.9\t3.0\t1.4\t0.2\t0\n",
    "    2\t4.7\t3.2\t1.3\t0.2\t0\n",
    "    3\t4.6\t3.1\t1.5\t0.2\t0\n",
    "    4\t5.0\t3.6\t1.4\t0.2\t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Implement PCA\n",
    "\n",
    "Implement and test the following methods necessary to perform PCA in `pca_cov.py`.\n",
    "\n",
    "- `covariance_matrix`: Computes the covariance matrix of data\n",
    "- `compute_prop_var`: Computes the proportion variance accounted for by the principal components (PCs).\n",
    "- `compute_cum_var`: Computes the *cumulative* proportion variance accounted for by the PCs.\n",
    "- `pca`: Method to perform PCA on the data\n",
    "- `elbow_plot` (**answer Question 1**)\n",
    "- `pca_project`: Project the data into PCA space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Test `covariance_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test covariance here\n",
    "np.random.seed(0)\n",
    "d = np.random.randn(100, 3)\n",
    "cov_mat = pca.covariance_matrix(d)\n",
    "print(f'Your covariance matrix has shape {cov_mat.shape} and should be (3, 3)')\n",
    "print(f'Your covariance matrix is:\\n{cov_mat} and should be\\n[[ 1.06338 -0.07562  0.11267]\\n [-0.07562  0.97412 -0.0222 ]\\n [ 0.11267 -0.0222   0.96217]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Test `prop_var`\n",
    "\n",
    "Takes eigenvalues ordered large-to-small and computes the proportion of the total variance account for by the $k^{th}$ principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prop_var here\n",
    "np.random.seed(0)\n",
    "test_evals = np.sort(np.random.uniform(size=(10,)))[::-1]\n",
    "prop_var = pca.compute_prop_var(test_evals)\n",
    "print(f'Your list is actually a Python list (as it should be)? {isinstance(prop_var, list)}')\n",
    "print(f'Your proportion variance list length is {len(prop_var)} and should be 10')\n",
    "print(f'Your proportion variance list begins with\\n{prop_var[:2]} and it should be\\n[0.15649813681155653, 0.1448232917174111]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Test `compute_cum_var`\n",
    "\n",
    "Takes proportion variance for principal components, ordered large-to-small, and computes the cumulative sum (cumulative variance accounted for by the first $k$ principal components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accum_var here\n",
    "np.random.seed(0)\n",
    "test_evals = np.sort(np.random.uniform(size=(10,)))[::-1]\n",
    "prop_var = pca.compute_prop_var(test_evals)\n",
    "accum_var = pca.compute_cum_var(prop_var)\n",
    "print(f'Your list is actually a Python list (as it should be)? {isinstance(accum_var, list)}')\n",
    "print(f'Your cumulative variance list length is {len(accum_var)} and should be 10')\n",
    "print(f'Your cumulative variance list begins with\\n{accum_var[:2]} and should be\\n[0.15649813681155653, 0.3013214285289676]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) Test `pca`\n",
    "\n",
    "Performs PCA using the covariance matrix method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pca (no normalization) here\n",
    "iris_headers = list(iris_data.columns[:-1])\n",
    "pca.pca(iris_headers)\n",
    "\n",
    "# test that instance variable shape are correct\n",
    "print(f'There are {len(pca.vars)} vars in Iris PCA and there should be 4.')\n",
    "print(f'The original PCA data has shape {pca.A.shape} and should be (150, 4).')\n",
    "print(f'Eigenvector shape: {pca.e_vecs.shape} should be (4, 4).\\nEigenvalue shape: {pca.e_vals.shape} should be (4,).')\n",
    "print(f'Length of proportion variance account for: {len(pca.get_prop_var())} should be 4.')\n",
    "print(f'Length of cumulative proportion variance account for: {len(pca.get_cum_var())} should be 4.')\n",
    "print()\n",
    "\n",
    "# Test values\n",
    "print(f\"Your vars in Iris PCA:\\n{pca.vars}  and they should be\\n['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth']\")\n",
    "print(f'Your eigenvectors:\\n{pca.e_vecs}. They should be\\n[[ 0.36139 -0.65659 -0.58203  0.31549]\\n [-0.08452 -0.73016  0.59791 -0.31972].\\n [ 0.85667  0.17337  0.07624 -0.47984]\\n[ 0.35829  0.07548  0.54583  0.75366]].')\n",
    "print(f'Your eigenvalues:\\n{pca.e_vals}. They should be\\n[4.22824 0.24267 0.07821 0.02384]')\n",
    "print(f'Cumulative proportion variance account for:\\n{pca.get_cum_var()}. It should be\\n[0.924618723201727, 0.9776852063187949, 0.9947878161267245, 0.9999999999999999] .')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pca (normalization) here\n",
    "iris_headers = list(iris_data.columns[:-1])\n",
    "pca.pca(iris_headers, normalize=True)\n",
    "\n",
    "# test that instance variable shape are correct\n",
    "print(f'There are {len(pca.vars)} vars in Iris PCA and there should be 4.')\n",
    "print(f'The original PCA data has shape {pca.A.shape} and should be (150, 4).')\n",
    "print(f'Eigenvector shape: {pca.e_vecs.shape} should be (4, 4).\\nEigenvalue shape: {pca.e_vals.shape} should be (4,).')\n",
    "print(f'Length of proportion variance account for: {len(pca.get_prop_var())} should be 4.')\n",
    "print(f'Length of cumulative proportion variance account for: {len(pca.get_cum_var())} should be 4.')\n",
    "print(f'Data min/max is {pca.A.min()}/{pca.A.max()} should be 0.0/1.0')\n",
    "print()\n",
    "\n",
    "# Some test values\n",
    "print(f'Your eigenvalues:\\n{pca.e_vals}. They should be\\n[0.23245 0.03247 0.0096  0.00176].')\n",
    "print(f'Cumulative proportion variance account for:\\n{pca.get_cum_var()}. It should be\\n[0.8413603821315434, 0.9588784639918418, 0.9936140780797744, 1.0].')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (v) Test `elbow_plot`\n",
    "\n",
    "Visualize the cumulative proportion variance accounted for by the first $k$ principal components.\n",
    "\n",
    "**Make sure that you have the normalized PCA in memory before proceeding (the last cell of test code above)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test elbow plot\n",
    "pca.elbow_plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Based on the Iris elbow plot, how many principle components would you drop. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (vi) Test `pca_project`\n",
    "\n",
    "Project the data onto a list of the top $2$ principal components (`pcs_to_keep = [0, 1]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = 2\n",
    "pcs_to_keep = np.arange(num_dims)\n",
    "iris_proj = pca.pca_project(pcs_to_keep)\n",
    "print(iris_proj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot your PCA projected Iris data\n",
    "\n",
    "- In the cell below, create a scatterplot of your PCA projected data.\n",
    "- Label the x and y axes appropriately.\n",
    "\n",
    "If everything goes well, you should see two distinct clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Dropping different pairs of principal components\n",
    "\n",
    "- In the cell below, create a \"high quality\" 2x2 subplots grid of scatterplots that drops different consective PCs from the data, then project onto the remaining PCs.\n",
    "\n",
    "The 2x2 plots should keep:\n",
    "- (top-left) PCs [2, 3]\n",
    "- (top-right) PCs [1, 2]\n",
    "- (bottom-left) PCs [3, 0]\n",
    "- (bottom-right) PCs [0, 1]\n",
    "\n",
    "High quality means\n",
    "- x and y axis label indicating the PC (e.g. PC1)\n",
    "- title indicating the PCs shown in the plot\n",
    "\n",
    "You may have to adjust the font/figure sizes to make things legiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Interpret what the 2x2 grid of plots tells us about keeping different PCs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e) Reconstruct data based on PCs\n",
    "\n",
    "- In `pca_cov.py`, implement `pca_then_project_back`, which projects the data to PCA space, keeping only the top $k$ PCs, then projects from PCA space back onto the original data space.\n",
    "- In the cell below, create a scatter plot of the two data variables `sepalLength` (x), `sepalWidth` (y) of the Iris data **without any normalization**.\n",
    "- In the 2nd cell below, do PCA **on the normalized Iris data** and create a 2x2 grid of scatter plots showing the data reconstruction of the 1st two data variables (`sepalLength`, `sepalWidth` — *these are what your axis labels should be*) when keeping the top 1, 2, 3, or 4 (all) principal components.\n",
    "    - If everything goes well, if you keep all 4 PCs you should get the original dataset back (it should match your 'sepalLength', 'sepalWidth' plot created one cell above — including the data range and center)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original iris data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x2 grid of scatter plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** How well does each of the re-projections preserve properties of the original data? Briefly interpret what the above 2x2 grid means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Eigenfaces. Perform PCA on face images\n",
    "\n",
    "Here, you will implement the Eigenface algorithm, which involves performing PCA on images of human faces. Here is a link to original paper: https://sites.cs.ucsb.edu/~mturk/Papers/jcn.pdf\n",
    "\n",
    "*Reference:* M. Turk; A. Pentland (1991). \"Eigenfaces for recognition\" (PDF). *Journal of Cognitive Neuroscience*. 3 (1): 71–86."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Load in LFWcrop face dataset\n",
    "\n",
    "Run the following cell to load in the face images and labels (which celebrity each face is of).\n",
    "\n",
    "More info about LFWcrop face dataset: http://conradsanderson.id.au/lfwcrop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_imgs = np.load('data/lfwcrop.npy')\n",
    "face_names = np.loadtxt('data/lfwcrop_ids.txt', dtype=str, delimiter='\\n')\n",
    "\n",
    "print(f'There are {len(face_imgs)} faces. There should be 13231.')\n",
    "print(f'Shape of images is are {face_imgs.shape} faces. It should be (13231, 64, 64).')\n",
    "print(f'Names of faces match the number of face images? {np.all(len(face_names) == len(face_imgs))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Look at the face data\n",
    "\n",
    "- In the cell below, fill in the function to create a 5x5 subplots grid of plots. In each plot, show one of the first 25 images in the dataset. For the title of the plot, use the corresponding name of the celebrity whose face. For showing the image, use `imshow`: https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.imshow.html#matplotlib-pyplot-imshow\n",
    "- Run the function to create the plot of the faces below.\n",
    "- When you call `imshow`, add this optional parameter (otherwise faces look ghastly): `cmap=plt.get_cmap('gray')`\n",
    "- Turn off the x and y ticks (they just add clutter).\n",
    "- **Before moving on, make sure that the celebrity names match the image!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_plot(face_imgs, face_names):\n",
    "    '''Create a 5x5 grid of face images\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    face_imgs: ndarray. shape=(N, img_y, img_x).\n",
    "        Grayscale images to show.\n",
    "    face_names: ndarray. shape=(N,).\n",
    "        Names of the person in each image represented as strings.\n",
    "    \n",
    "    TODO:\n",
    "    - Create a 5x5 grid of plots of a legible size\n",
    "    - In each plot, show the grayscale image and make the title the person's name.\n",
    "    '''\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Visualize the \"mean face\"\n",
    "\n",
    "- Take the mean face image across the dataset and show it below.\n",
    "\n",
    "NOTE: Use the same `cmap` parameter as above and turn off x and y tick marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** What does the \"mean face\" image represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d) Preprocess face data\n",
    "\n",
    "In the face dataset, each person is a sample and each pixel is a variable. Currently, the face data has shape = `(13231, 64, 64)` or $N = 13231$ samples and $M = 64*64 = 4096$ pixels. To run PCA, we'll need to \\\"unravel\\\" each row of pixels and glue them into one big vector so that the shape = `(13231, 64*64)` = `(13231, 4096)`. This will make a standard matrix and should work with your PCA code.\n",
    "\n",
    "\n",
    "- Reshape the face data to make this so. **Do NOT hard code ANY part of the (13231, 64*64)!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your reshape code here\n",
    "\n",
    "print(f'Your reshaped face images have shape={face_imgs_vec.shape} and it should be (13231, 4096).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e) Perform PCA on preprocessed face data\n",
    "\n",
    "- Create pandas DataFrame object that holds the face data. The header names of consecutive integers `0, 1, ..., 4095` are fine.\n",
    "- Create `PCA_COV` object that holds the face DataFrame.\n",
    "- Run PCA. For `vars` pass in a Python list of ints (`0, 1, ..., 4095`).\n",
    "\n",
    "NOTE: This is not a \"toy\" dataset so it might take a few minutes to finish computing.\n",
    "- Use the python `time` module (i.e. `import time`) to time how long the `pca` method takes to finish processing the data, then print out the time elapsed below. If this takes more than a few minutes, something probably is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f) Make face PCA elbow plot\n",
    "\n",
    "- Make an elbow plot of the PCA results.\n",
    "- You should readily notice that it is challenging to see how many PCs are required before the curve plateaus. Update your plot below with a reasonable number of PCs to show in order to focus on the curve before it plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2g) Reconstruct faces using the principal components\n",
    "\n",
    "Use your elbow plot to find the number of top principal components required to *approximately* explain the following cumulative proportion of the variance:\n",
    "- 50%\n",
    "- 70%\n",
    "- 80%\n",
    "- 95%\n",
    "\n",
    "\n",
    "- For each of the above number of PCs, project them back onto the original data space.\n",
    "- Write then use the below `make_imgs` function to convert the projected data back to image shapes `(13231, 64, 64)`.\n",
    "- Use `face_plot` to plot the reconstructed faces for each of the variance numbers above.\n",
    "\n",
    "**There should be 4 5x5 plots below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_imgs(reconstucted_imgs):\n",
    "    '''Inflates a 1D feature vector representation of images into 2D images for plotting.\n",
    "    Assumes square images\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    reconstucted_imgs: ndarray. shape=(N, img_y*img_x)\n",
    "        1D vector representation of each image\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    ndarray. shape=(N, img_y, img_x)\n",
    "        The images inflated into 2D\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50% variance face grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 70% variance face grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 80% variance face grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 95% variance face grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Briefly interpret the face reconstruction with the above choices of proportion variance explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "To receive credit for any extension, you must:\n",
    "- Not modify / prevent any code from the core project from working (e.g. make a copy before changing). In other words, **the notebook test code should still work!**\n",
    "- **You must describe what you did and what you found in detail**. This includes a summary of parameter values used in your simulations.\n",
    "- Include (*labeled!*) plots and/or numbers to present your results.\n",
    "- Write up your extensions below or in a separate notebook.\n",
    "\n",
    "**Rule of thumb: one deep, thorough extension is worth more than several quick, shallow extensions!**\n",
    "\n",
    "**Reminder:** Give credit to all sources, including anyone that you consulted.\n",
    "\n",
    "**Note about CSV files:** Please try if at all possible to use pandas `read_csv` method with the URL argument in your submitted notebook extensions (should be possible if you downloaded your CSV file off the internet). This way, graders will be able to download the same data and you won't have to remember to copy over the CSV file with your project submission (this is welcome too). Remember the 10MB rule though for filer submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Your own dataset\n",
    "\n",
    "- Do PCA on a dataset of your choice. Use any of the canonical analysis tools (e.g. elbow plot, proportion variance explained, etc) to inform your analysis and exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Image compression\n",
    "\n",
    "- Run PCA on your own images and analyze the trade-off in compression and reconstruction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Facial recognition with Eigenfaces\n",
    "\n",
    "You can use Eigenfaces to perform facial recognition! Here is the idea:\n",
    "\n",
    "1. Perform PCA on images of faces of people who you would like to be able to \"learn\" to recognize (creating a \"database\" of known faces). This is just like what you did in the main project.\n",
    "2. Project a \"query\" image of a face that you would like to recognize into the PCA space established by the database of known faces (the number of PCA dimensions is up to you).  \n",
    "3. Project each of the images in the face database into the same PCA space established by the database of known faces (the number of PCA dimensions is the same as before).\n",
    "4. Treat the projected query image and each projected database images as vectors. Compute the Euclidean distance of the vector obtained by subtracting that of the query image and that of each database image.\n",
    "5. Pick a match tolerance level. If any of the distances is less than your tolerance level, you have a match! If none of the distances is smaller than your tolerance, you don't have a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
