{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAME HERE**\n",
    "\n",
    "Spring 2020\n",
    "\n",
    "CS 251B: Data Analysis and Visualization\n",
    "\n",
    "Project 3: Linear regression\n",
    "\n",
    "QR-based linear regression and polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data\n",
    "import linear_regression\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- In your implementations, only the following \"high level\" `scipy`/`numpy` functions can be used:\n",
    "    - `np.linalg.inv`\n",
    "    - `scipy.linalg.lstsq` (in `LinearRegression::linear_regression_scipy` only).\n",
    "    - `np.linalg.norm`\n",
    "    - (LA section only): `scipy.linalg.solve_triangular`, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implement a QR based linear regression solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a) QR-decomposition\n",
    "\n",
    "- Write algorithm to compute QR decomposition (`linear_regression::qr_decomposition`). **Run test code below.** Equation for R: $$R = Q^TA$$\n",
    "- Implement `linear_regression::linear_regression_qr` to use the QR decomposition to do the linear regression. Recall that the equation is $$Rc = Q^Ty$$ which can be solved without taking the inverse of $R$ through backsolving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test QR decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_filename = 'data/iris.csv'\n",
    "iris_data = data.Data(iris_filename)\n",
    "\n",
    "A = iris_data.select_data(['sepa_length', 'petal_width'])\n",
    "A1 = np.hstack([A, np.ones([len(A), 1])])\n",
    "\n",
    "lin_reg_qr = linear_regression.LinearRegression(iris_data)\n",
    "myQ, myR = lin_reg_qr.qr_decomposition(A1)\n",
    "\n",
    "Q, R = np.linalg.qr(A1)\n",
    "\n",
    "print('NOTE: It is ok if everything is GLOBALLY negated.\\n')\n",
    "print(f'Your Q shape is {myQ.shape} and should be {Q.shape}')\n",
    "print(f'Your R shape is {myR.shape} and should be {R.shape}')\n",
    "print(f'1st few rows of your Q are\\n{myQ[:3]} and should be\\n{Q[:3]}')\n",
    "print(f'\\nYour R is\\n{myR[:5]} and should be\\n{R[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test linear regression via QR decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_qr.linear_regression(['sepal_length'], 'petal_width', 'qr')\n",
    "lin_reg_qr.scatter('sepal_length', 'petal_width', 'qr')\n",
    "lin_reg_qr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** How do your results compare to the built-in SciPy solver? Is this what you expected? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4b) Compare performance of the linear regression methods\n",
    "\n",
    "- Implement `mean_sse` in `linear_regression.py`. **Run test code below**\n",
    "- In the cell below, load in the brain network `brain.csv` dataset.\n",
    "- Create `Data` and `LinearRegression` objects.\n",
    "- Load in the list of variable names in the brain dataset (62)\n",
    "- Do multiple linear regressions, separately for each of the linear regression methods that you have implemented (including `scipy`): Set all brain data variables except for the last one as the independent variables, the last variable is the depenendent variable.\n",
    "- Compute the mean sum of squares error (MSSE) in the predictions made by each linear regression model and the actual y values.\n",
    "\n",
    "**Equation for MSSE:** $$E = \\frac{1}{N}\\sum_{i=1}^N \\left (y_i - \\hat{y}_i \\right )^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Mean SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with iris data\n",
    "lin_reg_norm = linear_regression.LinearRegression(iris_data)\n",
    "lin_reg_norm.linear_regression(['sepal_length'], 'petal_width', 'normal')\n",
    "print(f'Your Iris mean SSE is {lin_reg_norm.mean_sse():.2f} and should be 0.19')\n",
    "\n",
    "# Test with passing in random data\n",
    "np.random.seed(0)\n",
    "test_data = np.random.rand(iris_data.get_num_samples(), 1)\n",
    "print(f'Your random data mean SSE is {lin_reg_norm.mean_sse(X=test_data):.2f} and should be 16.72')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** What are the mean sum of squares errors for each of the regression methods on the brain data? Which method(s) do best and which do the worst **and why**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been creating linear regression fits of form $y =c_0 + c_1x_1 + c_2x_2 + \\ldots$, where $x_i$ are independent variables (columns of $A$) and $c_i$ are corresponding coefficients in $c$. However, this equation only allows us to fit with a line/plane. This may not be the best choice for all datasets.\n",
    "\n",
    "In this task, you will generalize the linear regression model form to include higher-degree (>1) polynomial terms and explore how this may improve fits to complex data. For example, assume we're doing a single linear regression with independent variable $x_1$ and dependent variable $y$. A linear regression that fits data with a quadratic shape has the form$$y = c_0 + c_1x_1 + c_2x_1^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a) Implement polynomial regression\n",
    "\n",
    "- Implement two methods: `LinearRegression::poly_regression` and `LinearRegression::make_polynomial_matrix` (*helper method*). To implement polynomial regression, recall that you add one or more \"independent variables\" to $A$, derived from the original column vector $x_1$. For example, if $A$ originally has a column vector for the independent variable $\\vec{x_1}$ ($A = [\\vec{x_1}]$) and we wanted to make the above quadratic model, we would append $x_1^2$ ($A = [\\vec{x_1}, \\vec{x_1^2}]$).\n",
    "- Add support for plotting polynomials in `LinearRegression::scatter`. To do this, you are going to generalize the regression line to a regression polynomial (if `self.p > 1`).\n",
    "    - Getting your polynomial \"x\" values: Duplicate your line sample points for $p$ columns then raise each column to the appropriate power.\n",
    "    - Getting your polynomial \"y\" values: Use matrix multiplication with your polynomial regression model slopes and/or intercepts.\n",
    "- Update `predict` to run `make_polynomial_matrix` on `X` to change it if `X` is not `None` and `self.p > 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `make_polynomial_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = np.r_[1:10].reshape((9, 1))\n",
    "test_p = 3\n",
    "\n",
    "# Test cubic\n",
    "lin_reg = linear_regression.LinearRegression(data.Data())\n",
    "print(f'Your polynomial matrix:\\n{lin_reg.make_polynomial_matrix(test_A, 3)}')\n",
    "\n",
    "true_mat = '''\n",
    "[[  1.   1.   1.]\n",
    " [  2.   4.   8.]\n",
    " [  3.   9.  27.]\n",
    " [  4.  16.  64.]\n",
    " [  5.  25. 125.]\n",
    " [  6.  36. 216.]\n",
    " [  7.  49. 343.]\n",
    " [  8.  64. 512.]\n",
    " [  9.  81. 729.]]\n",
    "'''\n",
    "print('It should look like:\\n', true_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b) Fitting data with polynomial regression\n",
    "\n",
    "- In the cell below, load in `poly_data_1.csv`, make a `Data` object, make a `LinearRegression` object based off it like usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Run polynomial regression on `poly_data_1` and plot results with `p=1`\n",
    "\n",
    "- Run `poly_regression` with the polynomial degree $p = 1$. Solve using the normal equations.\n",
    "- Use `scatter` to plot the results below.\n",
    "- Print out the mean sum of squares error.\n",
    "\n",
    "The plot created by running the below cell should \"look right\" to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Higher degree polynomials\n",
    "\n",
    "In the cell below,\n",
    "- Run `poly_regression` on the same datset with `p = 5`.\n",
    "- Plot the results.\n",
    "- Print out the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Describe the fit compared to `p=1` — is it better or worse? Why?\n",
    "\n",
    "**Question 7:** Describe what happens as you experiment with the polynomial degree between 1 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 6:**\n",
    "\n",
    "**Answer 7:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Check overfitting\n",
    "\n",
    "Now you will check to see how the $p=5$ polynomial regression model generalizes to new data. \n",
    "\n",
    "- Load in `poly_data_2.csv`, which is the 2nd half of the `poly_data_1` dataset, make a new `Data` object, etc.\n",
    "- Run `scatter` to show a scatterplot and the polynomial regression of the `poly_data_2` data, but use the polynomial regression model fit to the `poly_data_1`. Do this by replacing the `Data` object in `LinearRegression` object with the one holding `poly_data_2`. Use `set_data()`; do NOT run `poly_regression` again. You want to use the same fitted slopes/intercept from `poly_data_1`.\n",
    "- Print out the mean sum of squared error obtained when predicting the `poly_data_2` using the regression model fit on `poly_data_1`. Do this by passing in the data column `X` selected from `poly_data_2` as a column vector ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:** Describe how the fitted coefficients obtained for `poly_data_1` do on `poly_data_2`. Do they do a good or bad job? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 8:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) Even higher degree polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, experiment with fitting and plotting `poly_data_1` again, but this time with an even higher degree polynomial `p=19`.\n",
    "\n",
    "Be sure to:\n",
    "- Fit the higher degree polynomial to the `poly_data_1` data.\n",
    "- Plot the results for both `poly_data_1` and `poly_data_2`.\n",
    "- Print out the MSSE for both `poly_data_1` and `poly_data_2`\n",
    "- Like you did with `p = 5`, experiment how the results transfer to `poly_data_2` (i.e. do NOT re-run `poly_regression` after fitting coefficients to `poly_data_1`.\n",
    "\n",
    "**Question 9:** What's going on with the higher degree polynomial fit and $R^2$?\n",
    "\n",
    "**Question 10:** Compare the results for `poly_data_1` and `poly_data_2`. Back up your observations with numbers (e.g. MSSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 9:**\n",
    "\n",
    "**Answer 10:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "To receive credit for any extension, you must:\n",
    "- Not modify / prevent any code from the core project from working (e.g. make a copy before changing). In other words, **the notebook test code should still work!**\n",
    "- **You must describe what you did and what you found in detail**. This includes a summary of parameter values used in your simulations.\n",
    "- Include (*labeled!*) plots and/or numbers to present your results.\n",
    "- Write up your extensions below or in a separate notebook.\n",
    "\n",
    "**Rule of thumb: one deep, thorough extension is worth more than several quick, shallow extensions!**\n",
    "\n",
    "**Reminder:** Give credit to all sources, including anyone that you consulted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Your own data\n",
    "\n",
    "- Run linear regression on datasets that interest you. Identify your hypotheses about the association between variables and test them out. Make plots and report all relevant metrics fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linear regression algorithm comparison\n",
    "\n",
    "- Compare the linear regression methods that you implemented on a dataset of your choice.\n",
    "- Research and implement matrix condition number. Find a dataset with a poor matrix condition number and then compare the regression methods. Which does best and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Confidence intervals or other kinds of error bars on linear regression plots\n",
    "\n",
    "- Add the option to plot 95% confidence intervals on the linear regression predictions in your plot functions (e.g. `scatter`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Overfitting\n",
    "\n",
    "- Run polynomial regression on other datasets. What degree polynomial works well? When do you overfit?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
