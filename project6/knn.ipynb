{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAME HERE**\n",
    "\n",
    "Spring 2021\n",
    "\n",
    "CS 251: Data Analysis and Visualization\n",
    "\n",
    "Project 6: Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6) Supervised learning\n",
    "\n",
    "The overall goal of this project is to implement an email spam filter to determine whether an email is spam (*spam*) or not (*ham*). You will implement and compare the performance of two supervised learning algorithms: **K Nearest Neighbors (KNN)** and **Naive Bayes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: K Nearest Neighbors (KNN) Classifier\n",
    "\n",
    "To start off the project, you will implement the **KNN classifier**, a bedrock, highly-versatile, nonparametric (i.e. *memory-based*) supervised learning algorithm. You will test out and experiment with KNN on a **multi-class spiral 2D dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Load and visualize spiral data\n",
    "\n",
    "- Below, load in both spiral datasets 1 (`spiral_train_1.csv`, `spiral_val_1.csv`) and 2 (`spiral_train_2.csv`, `spiral_val_2.csv`). Each training set has 4,000 samples and each validation set has 1,200 samples (*there is no test set for this development dataset*).\n",
    "- Create a 2x2 grid plot showing the train and validation data side-by-side in each version of the dataset.\n",
    "    - Be sure to label your subplots with informative titles (which datset are we looking at?).\n",
    "    - Color-code the points based on their class.\n",
    "    - Set the figure size to make everything clearly legible (not microscopic).\n",
    "\n",
    "#### Format of spiral data\n",
    "\n",
    "- Column 1: x coordinate of a 2D point (on a spiral).\n",
    "- Column 2: y coordinate of a 2D point (on a spiral).\n",
    "- Column 3: class. Which spiral arm does the point belong to? Labels: [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spiral 1 train (4000, 2), classes (4000,)\n",
      "Spiral 1 validation (1200, 2), classes (1200,)\n",
      "Spiral 2 train (4000, 2), classes (4000,)\n",
      "Spiral 2 validation (1200, 2), classes (1200,)\n"
     ]
    }
   ],
   "source": [
    "spiral_1_train = np.loadtxt('data/spiral_train_1.csv', skiprows=1, delimiter=',')\n",
    "spiral_1_val = np.loadtxt('data/spiral_val_1.csv', skiprows=1, delimiter=',')\n",
    "spiral_2_train = np.loadtxt('data/spiral_train_2.csv', skiprows=1, delimiter=',')\n",
    "spiral_2_val = np.loadtxt('data/spiral_val_2.csv', skiprows=1, delimiter=',')\n",
    "\n",
    "spiral_1_train_y = spiral_1_train[:, 2]\n",
    "spiral_1_val_y = spiral_1_val[:, 2]\n",
    "spiral_2_train_y = spiral_2_train[:, 2]\n",
    "spiral_2_val_y = spiral_2_val[:, 2]\n",
    "\n",
    "spiral_1_train = spiral_1_train[:, :2]\n",
    "spiral_1_val = spiral_1_val[:, :2]\n",
    "spiral_2_train = spiral_2_train[:, :2]\n",
    "spiral_2_val = spiral_2_val[:, :2]\n",
    "\n",
    "print(f'Spiral 1 train {spiral_1_train.shape}, classes {spiral_1_train_y.shape}')\n",
    "print(f'Spiral 1 validation {spiral_1_val.shape}, classes {spiral_1_val_y.shape}')\n",
    "print(f'Spiral 2 train {spiral_2_train.shape}, classes {spiral_2_train_y.shape}')\n",
    "print(f'Spiral 2 validation {spiral_2_val.shape}, classes {spiral_2_val_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Implement KNN\n",
    "\n",
    "Implement the following methods in `knn.py`. Test relevant methods using the test code below.\n",
    "\n",
    "- Constructor\n",
    "- `train(data, y)`: Train the KNN classifier on the data `data`, where training samples have corresponding class labels in `y`.\n",
    "- `predict(data, k)`: Use the trained KNN classifier to predict the class label of each test sample in `data`. Determine class by voting: find the closest `k` training exemplars (training samples) and the class is the majority vote of the classes of these training exemplars.\n",
    "- `accuracy(y, y_pred)`: Compute percent correct given true data labels `y` and algorithm predicted labels `y_pred`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knn import KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.06 and should be 0.06.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "test_y = np.random.randint(low=0, high=11, size=(50,))\n",
    "test_y_pred = np.random.randint(low=0, high=11, size=(50,))\n",
    "\n",
    "classifier = KNN(num_classes=0)\n",
    "acc = classifier.accuracy(test_y, test_y_pred)\n",
    "print(f'Test accuracy is {acc} and should be 0.06.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: 1-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your accuracy with K=1 is 1.0 and should be 1.0\n"
     ]
    }
   ],
   "source": [
    "n_classes = 4\n",
    "classifier = KNN(num_classes=n_classes)\n",
    "classifier.train(spiral_1_train, spiral_1_train_y)\n",
    "\n",
    "k = 1\n",
    "spiral_1_y_pred = classifier.predict(spiral_1_train, k)\n",
    "acc = classifier.accuracy(y=spiral_1_train_y, y_pred=spiral_1_y_pred)\n",
    "print(f'Your accuracy with K=1 is {acc} and should be 1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Explain why in the above test, the accuracy must be 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2-KNN\n",
    "\n",
    "*Note: The below test code assumes that you resolve voting ties with the class that has a lower index. There is a numpy function that you may feel inclined to use (or not!) that handles this automatically.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your accuracy with K=2 is 0.88 and should be 0.88\n",
      "The mismatches between your predicted class of validation samples with indices 750-900 and the expected values are\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "Seeing all 0s means everything seems to be working great!\n"
     ]
    }
   ],
   "source": [
    "n_classes = 4\n",
    "classifier = KNN(num_classes=n_classes)\n",
    "classifier.train(spiral_1_train, spiral_1_train_y)\n",
    "\n",
    "k = 2\n",
    "spiral_1_y_pred = classifier.predict(spiral_1_val, k)\n",
    "acc = classifier.accuracy(y=spiral_1_val_y, y_pred=spiral_1_y_pred)\n",
    "print(f'Your accuracy with K=2 is {acc:.2f} and should be 0.88')\n",
    "\n",
    "true_test_y = np.array([2., 2., 2., 2., 2., 3., 2., 2., 3., 2., 2., 1., 2., 2., 2., 2., 2.,\n",
    "       2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 2.,\n",
    "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
    "       2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 3., 3., 2.,\n",
    "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 2., 3., 2., 2.,\n",
    "       2., 2., 2., 2., 3., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2.,\n",
    "       2., 2., 2., 3., 3., 2., 3., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
    "       2., 2., 2., 2., 2., 1., 3., 2., 2., 2., 3., 3., 2., 2., 2., 2., 2.,\n",
    "       2., 2., 2., 2., 3., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
    "\n",
    "print(f'The mismatches between your predicted class of validation samples with indices 750-900 and the expected values are\\n{np.where(true_test_y != spiral_1_y_pred[750:900], 1, 0)}')\n",
    "print('Seeing all 0s means everything seems to be working great!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Find the best `k`\n",
    "\n",
    "- Below, \"script\" your `predict` method on both spiral datasets 1 and 2. Compute the accuracy on the respective test sets with many different values of `k`.\n",
    "- Create two well-labeled plots, one for each spiral dataset, showing the accuracy for many different `k` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** What is the `k` that results in the highest accuracy on each spiral dataset?\n",
    "\n",
    "**Question 3:** Give at least one \"good\" reason why the accuracies are so different across the datasets. (*Hint: look at the data*)\n",
    "\n",
    "**Question 4:** Give at least one \"good\" reason why the best `k` values are so different across the datasets.\n",
    "\n",
    "**Question 5:** Is it a good idea to always set `k` to one of these values when working with another dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:**\n",
    "\n",
    "**Answer 3:** \n",
    "\n",
    "**Answer 4:** \n",
    "\n",
    "**Answer 5:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d) Visualize class boundaries\n",
    "\n",
    "- Implement `plot_predictions` in `knn.py` to visualize how different regions of the (2D) dataspace would be classified. In this visualization, use four discrete colors to represent each of the classes. For example, if KNN would classify (x, y) = (10, 10) to spiral 2, you would color that region blue (for example). You will repeat this for lots of different regularly spaced x,y points to get a better picture of the regions that would be predicted to belong to different classes.\n",
    "- For spiral dataset 1 and 2, plot the class boundaries for the k best value determined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Why could visualizing the class boundaries be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 6:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Spam email preprocessing pipeline\n",
    "\n",
    "Before you can build a spam email filter, you need to transform the email data into a suitable format so that KNN or other supervised learning algorithms can process them (this is called **preprocessing**).\n",
    "\n",
    "In this project, you will work with the **Enron email dataset**, a large datset consisting of ~34,000 emails. Enron is an energy company that famously went bankrupt in the early 2000s after committing massive accounting fraud (more info: https://en.wikipedia.org/wiki/Enron). The US government seized company emails during their investigation and they were released to the public much later and nowadays is a commonly used datset in machine learning. \n",
    "\n",
    "Your eventual goal will be to train a supervised learning algorithm on some of the emails and predict whether the remaining ones are spam or not.\n",
    "\n",
    "But first...onto the preprocessing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall preprocessing strategy\n",
    "\n",
    "We need to turn each email's text into something an algorithm can process (**features**). We will use a simple type of feature: **bag of words counts**. That is, we will reduce an email into a vector of how many times words appeared in it.\n",
    "\n",
    "*Problem:* There are too many words across all the emails. Processing the counts in each email would take too long. For example, there are more than 40,000 words across all the emails. If we were trying to predict whether 1,000 emails are spam or not, we would need to build a `1000 x 40000` matrix (count each of the 40,000 words in each of the 1,000 emails), which would take a very long time to process by the supervised learning algorithm. \n",
    "\n",
    "A work-around that works quite well is to restrict ourselves to the most frequent $W$ words in the email dataset. You can experiment with how many words to include (e.g. as an extension), but for concreteness we will set this $W=200$ in the core project. In the above example, we can then process `1000 x 200` matrix much more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Determine email word frequency\n",
    "\n",
    "The large size of the enron email datset makes the debugging process cumbersome. In situations like this, it is common to work with a **development dataset** — a mini version of the full dataset that is much faster to work with. The enron dev datset has 2 ham emails and 3 spam emails. \n",
    "\n",
    "- Download and extract the **Enron dev** emails. You should see a base `enron` folder, with `spam` and `ham` subfolders (these are the 2 classes), and documents in each with the raw email text. There should be 2 files in the ham folder and 3 files in the spam folder.\n",
    "- In `email_preprocessor.py` implement `count_words(email_path)` to build up a python dictionary of all the words in the dataset (keys) and their associated counts (values).\n",
    "- Write `find_top_words(word_freq)` to parse the dictionary and determine the top $W$ words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email_preprocessor as epp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `count_words` and `find_top_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq, num_emails = epp.count_words(email_path='data/enron_dev/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You found 5 emails in the datset. You should have found 5.\n"
     ]
    }
   ],
   "source": [
    "print(f'You found {num_emails} emails in the datset. You should have found 5.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You found 19/19 words.\n",
      "Your top 2 words are\n",
      "['subject', 'you']\n",
      "and they should be\n",
      "['subject', 'you']\n",
      "The counts of all the words are\n",
      "[5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "and they should be\n",
      "[5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "The 19 words should are\n",
      "['subject', 'you', 'can', 'be', 'smart', 'love', 'ecards', 'get', 'that', 'new', 'car', 'now', 're', 'rankings', 'thank', 'christmas', 'tree', 'farm', 'pictures']\n",
      " and they should be \n",
      "['subject', 'you', 'get', 'that', 'new', 'car', 'now', 'can', 'be', 'smart', 'love', 'ecards', 'christmas', 'tree', 'farm', 'pictures', 're', 'rankings', 'thank']\n",
      "with the last 17 words in any order (because their counts are tied)\n"
     ]
    }
   ],
   "source": [
    "top_words, top_counts = epp.find_top_words(word_freq)\n",
    "print(f\"You found {len(top_words)}/19 words.\")\n",
    "print(f\"Your top 2 words are\\n{top_words[:2]}\\nand they should be\\n['subject', 'you']\")\n",
    "print(f\"The counts of all the words are\\n{top_counts}\\nand they should be\\n[5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\")\n",
    "print(f\"The 19 words should are\\n{top_words}\\n and they should be \\n['subject', 'you', 'get', 'that', 'new', 'car', 'now', 'can', 'be', 'smart', 'love', 'ecards', 'christmas', 'tree', 'farm', 'pictures', 're', 'rankings', 'thank']\\nwith the last 17 words in any order (because their counts are tied)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Make feature vectors based only on the top word counts\n",
    "\n",
    "- Implement `make_feature_vectors`: Go back through the email folder structure and parse each email again. Now only count the frequency of words that are in the top $W$ word list. Keep track of whether each of these feature vectors are associated with a spam or not spam email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: PYDEVD_USE_CYTHON environment variable is set to 'NO'. Frame evaluator will be also disabled because it requires Cython extensions to be enabled in order to operate correctly.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-b97df4434ae3>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# pydev_debug_cell\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mhard_code_words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'subject'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'you'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'get'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'that'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'new'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'car'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'now'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'can'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'be'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'smart'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'love'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'ecards'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'christmas'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'tree'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'farm'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pictures'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m're'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rankings'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'thank'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mfeatures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mepp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake_feature_vectors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhard_code_words\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_emails\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memail_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'data/enron_dev/'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/cs251/project6/email_preprocessor.py\u001B[0m in \u001B[0;36mmake_feature_vectors\u001B[0;34m(top_words_to_find, num_emails, email_path, multi_thread, testTime)\u001B[0m\n\u001B[1;32m    172\u001B[0m         \u001B[0mtext_from_files\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mread_and_tokenize_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mall_file_paths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 174\u001B[0;31m         \u001B[0mflattened_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitertools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_iterable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext_from_files\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    175\u001B[0m         \u001B[0mword_freq\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcollections\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCounter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflattened_text\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/cs251/project6/email_preprocessor.py\u001B[0m in \u001B[0;36mmake_feature_vectors\u001B[0;34m(top_words_to_find, num_emails, email_path, multi_thread, testTime)\u001B[0m\n\u001B[1;32m    172\u001B[0m         \u001B[0mtext_from_files\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mread_and_tokenize_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mall_file_paths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 174\u001B[0;31m         \u001B[0mflattened_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitertools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_iterable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext_from_files\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    175\u001B[0m         \u001B[0mword_freq\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcollections\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCounter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflattened_text\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/snap/pycharm-professional/242/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mtrace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m                 \u001B[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    746\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpydev_state\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mSTATE_SUSPEND\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 747\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    748\u001B[0m                     \u001B[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    749\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrace_dispatch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/snap/pycharm-professional/242/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_args\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[0;31m# IFDEF CYTHON\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/snap/pycharm-professional/242/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1147\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/snap/pycharm-professional/242/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1160\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1162\u001B[0;31m                 \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "hard_code_words = ['subject', 'you', 'get', 'that', 'new', 'car', 'now', 'can', 'be', 'smart', 'love', 'ecards', 'christmas', 'tree', 'farm', 'pictures', 're', 'rankings', 'thank']\n",
    "features, y = epp.make_feature_vectors(hard_code_words, num_emails, email_path='data/enron_dev/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstSpamWordCounts = np.array([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "\n",
    "print(f'Your matrix of features has shape:\\n{features.shape}\\nand it should be\\n(5, 19).')\n",
    "print(f'Your class label vector has shape:\\n{y.shape}\\nand it should be\\n(5,).')\n",
    "print('\\nBelow, one number should be 3, the other should be 2.')\n",
    "print(f'Number of emails of class 0: {np.sum(y == 0)}\\nNumber of emails of class 1: {np.sum(y == 1)}')\n",
    "\n",
    "inds = np.arange(len(features))\n",
    "test_ind = inds[np.all(firstSpamWordCounts == features, axis=1)]\n",
    "print(f'\\nYour vector for 2958.2004-11-03.GP.spam.txt matches expected counts?\\n{len(test_ind) == 1}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Make train and test splits of the dataset\n",
    "\n",
    "Your matrix of features is for the entire dataset. We can't train the classifier on all these because then we won't have any emails left over to see how well your model's ability to discriminate spam/ham email generalizes to emails not seen during training!\n",
    "\n",
    "Implement `make_train_test_sets` to divide the email features into a 80/20 train/test split (80% of data used to train the supervised learning model, 20% we withhold and use for testing / prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x_train, y_train, inds_train, x_test, y_test, inds_test = epp.make_train_test_sets(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shapes for train/test splits:')\n",
    "print(f'Train {x_train.shape}, classes {y_train.shape}')\n",
    "print(f'Test {x_test.shape}, classes {y_test.shape}')\n",
    "print('\\nThey should be:\\nTrain (4, 19), classes (4,)\\nTest (1, 19), classes (1,)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}